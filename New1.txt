import org.apache.spark.sql.functions._
import org.apache.spark.sql.{SparkSession, Row}
import scala.collection.mutable.ArrayBuffer

object AddressMatching {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Address Matching")
      .master("local[*]")
      .getOrCreate()

    // Input path
    val inputPath = "/edl/hdfs/jffv-mns/testaddress/input/query-impala-88364.csv"
    val data = spark.read.option("header", "true").csv(inputPath)

    // Replace consecutive commas with a single comma
    val cleanCommasUDF = udf((address: String) => {
      Option(address).map(_.replaceAll(",{2,}", ",")).getOrElse("")
    })

    val cleanedDf = data.withColumn("cleaned_address", cleanCommasUDF(col("address")))
      .withColumn("normalized_postal_cd", col("postal_cd").substr(1, 5))

    // Tokenize and remove stopwords
    val tokenizer = new org.apache.spark.ml.feature.Tokenizer()
      .setInputCol("cleaned_address")
      .setOutputCol("tokens")
    val tokenizedDf = tokenizer.transform(cleanedDf)

    val remover = new org.apache.spark.ml.feature.StopWordsRemover()
      .setInputCol("tokens")
      .setOutputCol("filtered_tokens")
    val filteredDf = remover.transform(tokenizedDf)

    // Group by postal code and create address groups
    val groupedDf = filteredDf.groupBy("normalized_postal_cd").agg(
      collect_list(struct("cleaned_address", "filtered_tokens")).as("address_group")
    )

    // UDF to compute Jaccard similarity
    val similarityUDF = udf((addresses: Seq[Row]) => {
      val results = ArrayBuffer[(String, String, Double)]()
      for (i <- addresses.indices; j <- i + 1 until addresses.size) {
        val addr1 = addresses(i)
        val addr2 = addresses(j)
        val tokens1 = addr1.getAs[Seq[String]]("filtered_tokens")
        val tokens2 = addr2.getAs[Seq[String]]("filtered_tokens")
        val intersection = tokens1.intersect(tokens2).size.toDouble
        val union = tokens1.union(tokens2).distinct.size.toDouble
        val score = if (union > 0) intersection / union else 0.0
        results += ((addr1.getAs[String]("cleaned_address"), addr2.getAs[String]("cleaned_address"), score))
      }
      results
    })

    // Compute comparisons
    val comparisonDf = groupedDf.withColumn("comparisons", similarityUDF(col("address_group")))

    // Flatten comparisons and create a new DataFrame with address pairs and their similarity score
    val flatDf = comparisonDf
      .select(explode(col("comparisons")).as("comparison"))
      .select(
        col("comparison._1").as("address_1"),
        col("comparison._2").as("address_2"),
        col("comparison._3").as("similarity_score")
      )

    // Filter duplicates: similarity_score = 1.0 (identical addresses)
    val duplicateDf = flatDf.filter(col("similarity_score") === 1.0)

    // Flatten duplicates to avoid multiple counting
    val flattenedDuplicates = duplicateDf
      .select("address_1")
      .union(duplicateDf.select("address_2"))
      .distinct()
      .toDF("cleaned_address")

    // Identify unique records (addresses not in duplicates)
    val nonDuplicateDf = cleanedDf.join(
      flattenedDuplicates,
      cleanedDf("cleaned_address") === flattenedDuplicates("cleaned_address"),
      "left_anti"
    )

    // Combine unique and flattened duplicates
    val finalDf = nonDuplicateDf
      .select("cleaned_address")
      .union(flattenedDuplicates)
      .distinct()

    // Validate if any records are missing
    val missingRecordsDf = cleanedDf
      .select("cleaned_address")
      .join(finalDf, Seq("cleaned_address"), "left_anti")

    if (missingRecordsDf.count() > 0) {
      println(s"Missing Records Count: ${missingRecordsDf.count()}")
      missingRecordsDf.show(false)
    } else {
      println("No missing records. All records are accounted for.")
    }

    // Output paths
    val similarityOutputPath = "/edl/hdfs/jffv-mns/testaddress/output/address_similarity.csv"
    val uniqueAddressOutputPath = "/edl/hdfs/jffv-mns/testaddress/output/unique_addresses.csv"
    val duplicateAddressOutputPath = "/edl/hdfs/jffv-mns/testaddress/output/all_duplicates.csv"

    // Write address similarity results
    flatDf.write
      .mode("overwrite")
      .option("header", "true")
      .csv(similarityOutputPath)

    // Write unique address records
    nonDuplicateDf
      .select("cleaned_address", "postal_cd")
      .write.mode("overwrite")
      .option("header", "true")
      .csv(uniqueAddressOutputPath)

    // Write all duplicate records
    flattenedDuplicates.write
      .mode("overwrite")
      .option("header", "true")
      .csv(duplicateAddressOutputPath)

    // Count records
    val inputCount = data.count()
    val uniqueCount = nonDuplicateDf.count()
    val duplicateCount = flattenedDuplicates.count()
    val finalCount = finalDf.count()

    // Print record counts
    println(s"Input Records: $inputCount")
    println(s"Unique Records: $uniqueCount")
    println(s"Flattened Duplicate Records: $duplicateCount")
    println(s"Final Reconstructed Records: $finalCount")

    // Validate record consistency
    assert(finalCount == inputCount, "Final record count does not match input count!")
    println(s"Address similarity results have been written to $similarityOutputPath")
    println(s"Unique address records have been written to $uniqueAddressOutputPath")
    println(s"All duplicate records have been written to $duplicateAddressOutputPath")
  }
}


import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.ml.feature.HashingTF
import org.apache.spark.ml.linalg.SparseVector

// Initialize SparkSession
val spark = SparkSession.builder.appName("Address Deduplication and Standardization").getOrCreate()

// Load the data from the CSV file
val filePath = "mns/address/input/Site_Addrress_Validation-Dashboard_summary_old_data.csv"
val rawData = spark.read.option("header", "true").csv(filePath)

// Step 1: Add unique row_id using monotonically_increasing_id()
// Repartition to control the number of partitions and IDs
val rawDataWithID = rawData.repartition(1).withColumn("row_id", monotonically_increasing_id())

// Step 2: Remove extra commas from the address field
val cleanAddressUDF = udf((address: String) => address.replaceAll(",+", ",").trim)
val cleanedData = rawDataWithID.withColumn("cleaned_address", cleanAddressUDF(col("address")))

// Step 3: Extract numeric and non-numeric portions of the address
val extractNumbersUDF = udf((address: String) => address.replaceAll("[^0-9]", ""))
val extractNonNumbersUDF = udf((address: String) => address.replaceAll("[0-9]", "").replaceAll(",", " ").trim.toLowerCase)

val extractedData = cleanedData
  .withColumn("address_digits", extractNumbersUDF(col("cleaned_address")))
  .withColumn("address_non_digits", extractNonNumbersUDF(col("cleaned_address")))
  .select("row_id", "postal_cd", "cleaned_address", "address_digits", "address_non_digits")

// Step 4: Split the non-numeric portion into tokens
val splitTokensUDF = udf((address: String) => address.split(" ").map(_.trim).filter(_.nonEmpty))
val tokenizedData = extractedData.withColumn("address_tokens", splitTokensUDF(col("address_non_digits")))

// Step 5: Standardize suffixes using a predefined mapping
val suffixMapping = Map(
  "ALLEE" -> "ALY", "ALLEY" -> "ALY", "ALLY" -> "ALY", "ANEX" -> "ANX", "ANNEX" -> "ANX",
  "ARC" -> "ARC", "ARCADE" -> "ARC", "AV" -> "AVE", "AVE" -> "AVE", "AVEN" -> "AVE",
  "AVENU" -> "AVE", "AVENUE" -> "AVE", "AVN" -> "AVE", "AVNUE" -> "AVE", "BAYOO" -> "BYU",
  "BAYOU" -> "BYU", "BCH" -> "BCH", "BEACH" -> "BCH", "BEND" -> "BND", "BLF" -> "BLF",
  "BLUF" -> "BLF", "BLUFF" -> "BLF", "BLUFFS" -> "BLFS", "BOT" -> "BTM", "BOTTM" -> "BTM",
  "BOTTOM" -> "BTM", "BLVD" -> "BLVD", "BOUL" -> "BLVD", "BOULEVARD" -> "BLVD", "BOULV" -> "BLVD"
  // Add more mappings as needed
)

val standardizeSuffixUDF = udf((address: String) => {
  address.split(" ").map(word => suffixMapping.getOrElse(word.toUpperCase, word)).mkString(" ")
})

val standardizedData = tokenizedData.withColumn("standardized_address", standardizeSuffixUDF(col("address_non_digits")))

// Step 6: Use HashingTF for feature extraction
val hashingTF = new HashingTF()
  .setInputCol("address_tokens")
  .setOutputCol("rawFeatures")
  .setNumFeatures(1000)

val featurizedData = hashingTF.transform(standardizedData)

// Step 7: Define cosine similarity function
def cosineSimilarity(vec1: Array[Double], vec2: Array[Double]): Double = {
  val dotProduct = vec1.zip(vec2).map { case (x, y) => x * y }.sum
  val normA = math.sqrt(vec1.map(x => x * x).sum)
  val normB = math.sqrt(vec2.map(x => x * x).sum)
  if (normA > 0 && normB > 0) dotProduct / (normA * normB) else 0.0
}

// Step 8: UDF for cosine similarity calculation
val cosineSimilarityUDF = udf((vec1: SparseVector, vec2: SparseVector) => cosineSimilarity(vec1.toArray, vec2.toArray))

// Step 9: Self-join for comparison
val joinedData = featurizedData.as("a")
  .join(featurizedData.as("b"), col("a.postal_cd") === col("b.postal_cd") && col("a.row_id") =!= col("b.row_id"))

// Step 10: Filter matches based on numeric and text similarity
val similarityData = joinedData
  .filter(col("a.address_digits") === col("b.address_digits")) // First check: numeric parts match
  .select(
    col("a.row_id").alias("RowID1"),
    col("b.row_id").alias("RowID2"),
    col("a.cleaned_address").alias("Address1"),
    col("b.cleaned_address").alias("Address2"),
    cosineSimilarityUDF(col("a.rawFeatures"), col("b.rawFeatures")).alias("SimilarityScore")
  )

// Step 11: Separate high and low similarity records
val highSimilarityData = similarityData.filter(col("SimilarityScore") > 0.7)
val lowSimilarityData = similarityData.filter(col("SimilarityScore") <= 0.7)

// Step 12: Show and save results
highSimilarityData.show(truncate = false)
lowSimilarityData.show(truncate = false)

// Save high similarity records
highSimilarityData.coalesce(1).write.mode("overwrite").csv("/edl/hdfs/address/high_similarity_output")

// Save low similarity records
lowSimilarityData.coalesce(1).write.mode("overwrite").csv("/edl/hdfs/address/low_similarity_output")
