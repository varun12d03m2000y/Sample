import org.apache.spark.sql.functions._
import org.apache.spark.sql.{SparkSession, Row}
import scala.collection.mutable.ArrayBuffer

object AddressMatching {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Address Matching")
      .master("local[*]")
      .getOrCreate()

    // Input path
    val inputPath = "/edl/hdfs/jffv-mns/testaddress/input/query-impala-88364.csv"
    val data = spark.read.option("header", "true").csv(inputPath)

    // Replace consecutive commas with a single comma
    val cleanCommasUDF = udf((address: String) => {
      Option(address).map(_.replaceAll(",{2,}", ",")).getOrElse("")
    })

    val cleanedDf = data.withColumn("cleaned_address", cleanCommasUDF(col("address")))
      .withColumn("normalized_postal_cd", col("postal_cd").substr(1, 5))

    // Tokenize and remove stopwords
    val tokenizer = new org.apache.spark.ml.feature.Tokenizer()
      .setInputCol("cleaned_address")
      .setOutputCol("tokens")
    val tokenizedDf = tokenizer.transform(cleanedDf)

    val remover = new org.apache.spark.ml.feature.StopWordsRemover()
      .setInputCol("tokens")
      .setOutputCol("filtered_tokens")
    val filteredDf = remover.transform(tokenizedDf)

    // Group by postal code and create address groups
    val groupedDf = filteredDf.groupBy("normalized_postal_cd").agg(
      collect_list(struct("cleaned_address", "filtered_tokens")).as("address_group")
    )

    // UDF to compute Jaccard similarity
    val similarityUDF = udf((addresses: Seq[Row]) => {
      val results = ArrayBuffer[(String, String, Double)]()
      for (i <- addresses.indices; j <- i + 1 until addresses.size) {
        val addr1 = addresses(i)
        val addr2 = addresses(j)
        val tokens1 = addr1.getAs[Seq[String]]("filtered_tokens")
        val tokens2 = addr2.getAs[Seq[String]]("filtered_tokens")
        val intersection = tokens1.intersect(tokens2).size.toDouble
        val union = tokens1.union(tokens2).distinct.size.toDouble
        val score = if (union > 0) intersection / union else 0.0
        results += ((addr1.getAs[String]("cleaned_address"), addr2.getAs[String]("cleaned_address"), score))
      }
      results
    })

    // Compute comparisons
    val comparisonDf = groupedDf.withColumn("comparisons", similarityUDF(col("address_group")))

    // Flatten comparisons and create a new DataFrame with address pairs and their similarity score
    val flatDf = comparisonDf
      .select(explode(col("comparisons")).as("comparison"))
      .select(
        col("comparison._1").as("address_1"),
        col("comparison._2").as("address_2"),
        col("comparison._3").as("similarity_score")
      )

    // Filter duplicates: similarity_score = 1.0 (identical addresses)
    val duplicateDf = flatDf.filter(col("similarity_score") === 1.0)

    // Flatten duplicates to avoid multiple counting
    val flattenedDuplicates = duplicateDf
      .select("address_1")
      .union(duplicateDf.select("address_2"))
      .distinct()
      .toDF("cleaned_address")

    // Identify unique records (addresses not in duplicates)
    val nonDuplicateDf = cleanedDf.join(
      flattenedDuplicates,
      cleanedDf("cleaned_address") === flattenedDuplicates("cleaned_address"),
      "left_anti"
    )

    // Combine unique and flattened duplicates
    val finalDf = nonDuplicateDf
      .select("cleaned_address")
      .union(flattenedDuplicates)
      .distinct()

    // Validate if any records are missing
    val missingRecordsDf = cleanedDf
      .select("cleaned_address")
      .join(finalDf, Seq("cleaned_address"), "left_anti")

    if (missingRecordsDf.count() > 0) {
      println(s"Missing Records Count: ${missingRecordsDf.count()}")
      missingRecordsDf.show(false)
    } else {
      println("No missing records. All records are accounted for.")
    }

    // Output paths
    val similarityOutputPath = "/edl/hdfs/jffv-mns/testaddress/output/address_similarity.csv"
    val uniqueAddressOutputPath = "/edl/hdfs/jffv-mns/testaddress/output/unique_addresses.csv"
    val duplicateAddressOutputPath = "/edl/hdfs/jffv-mns/testaddress/output/all_duplicates.csv"

    // Write address similarity results
    flatDf.write
      .mode("overwrite")
      .option("header", "true")
      .csv(similarityOutputPath)

    // Write unique address records
    nonDuplicateDf
      .select("cleaned_address", "postal_cd")
      .write.mode("overwrite")
      .option("header", "true")
      .csv(uniqueAddressOutputPath)

    // Write all duplicate records
    flattenedDuplicates.write
      .mode("overwrite")
      .option("header", "true")
      .csv(duplicateAddressOutputPath)

    // Count records
    val inputCount = data.count()
    val uniqueCount = nonDuplicateDf.count()
    val duplicateCount = flattenedDuplicates.count()
    val finalCount = finalDf.count()

    // Print record counts
    println(s"Input Records: $inputCount")
    println(s"Unique Records: $uniqueCount")
    println(s"Flattened Duplicate Records: $duplicateCount")
    println(s"Final Reconstructed Records: $finalCount")

    // Validate record consistency
    assert(finalCount == inputCount, "Final record count does not match input count!")
    println(s"Address similarity results have been written to $similarityOutputPath")
    println(s"Unique address records have been written to $uniqueAddressOutputPath")
    println(s"All duplicate records have been written to $duplicateAddressOutputPath")
  }
}


import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.ml.feature.HashingTF
import org.apache.spark.ml.linalg.SparseVector

// Initialize SparkSession
val spark = SparkSession.builder.appName("Address Deduplication and Standardization").getOrCreate()

// Load the data from the CSV file
val filePath = "mns/address/input/Site_Addrress_Validation-Dashboard_summary_old_data.csv"
val rawData = spark.read.option("header", "true").csv(filePath)

// Step 1: Add unique row_id using monotonically_increasing_id()
// Repartition to control the number of partitions and IDs
val rawDataWithID = rawData.repartition(1).withColumn("row_id", monotonically_increasing_id())

// Step 2: Remove extra commas from the address field
val cleanAddressUDF = udf((address: String) => address.replaceAll(",+", ",").trim)
val cleanedData = rawDataWithID.withColumn("cleaned_address", cleanAddressUDF(col("address")))

// Step 3: Extract numeric and non-numeric portions of the address
val extractNumbersUDF = udf((address: String) => address.replaceAll("[^0-9]", ""))
val extractNonNumbersUDF = udf((address: String) => address.replaceAll("[0-9]", "").replaceAll(",", " ").trim.toLowerCase)

val extractedData = cleanedData
  .withColumn("address_digits", extractNumbersUDF(col("cleaned_address")))
  .withColumn("address_non_digits", extractNonNumbersUDF(col("cleaned_address")))
  .select("row_id", "postal_cd", "cleaned_address", "address_digits", "address_non_digits")

// Step 4: Split the non-numeric portion into tokens
val splitTokensUDF = udf((address: String) => address.split(" ").map(_.trim).filter(_.nonEmpty))
val tokenizedData = extractedData.withColumn("address_tokens", splitTokensUDF(col("address_non_digits")))

// Step 5: Standardize suffixes using a predefined mapping
val suffixMapping = Map(
  "ALLEE" -> "ALY", "ALLEY" -> "ALY", "ALLY" -> "ALY", "ANEX" -> "ANX", "ANNEX" -> "ANX",
  "ARC" -> "ARC", "ARCADE" -> "ARC", "AV" -> "AVE", "AVE" -> "AVE", "AVEN" -> "AVE",
  "AVENU" -> "AVE", "AVENUE" -> "AVE", "AVN" -> "AVE", "AVNUE" -> "AVE", "BAYOO" -> "BYU",
  "BAYOU" -> "BYU", "BCH" -> "BCH", "BEACH" -> "BCH", "BEND" -> "BND", "BLF" -> "BLF",
  "BLUF" -> "BLF", "BLUFF" -> "BLF", "BLUFFS" -> "BLFS", "BOT" -> "BTM", "BOTTM" -> "BTM",
  "BOTTOM" -> "BTM", "BLVD" -> "BLVD", "BOUL" -> "BLVD", "BOULEVARD" -> "BLVD", "BOULV" -> "BLVD"
  // Add more mappings as needed
)

val standardizeSuffixUDF = udf((address: String) => {
  address.split(" ").map(word => suffixMapping.getOrElse(word.toUpperCase, word)).mkString(" ")
})

val standardizedData = tokenizedData.withColumn("standardized_address", standardizeSuffixUDF(col("address_non_digits")))

// Step 6: Use HashingTF for feature extraction
val hashingTF = new HashingTF()
  .setInputCol("address_tokens")
  .setOutputCol("rawFeatures")
  .setNumFeatures(1000)

val featurizedData = hashingTF.transform(standardizedData)

// Step 7: Define cosine similarity function
def cosineSimilarity(vec1: Array[Double], vec2: Array[Double]): Double = {
  val dotProduct = vec1.zip(vec2).map { case (x, y) => x * y }.sum
  val normA = math.sqrt(vec1.map(x => x * x).sum)
  val normB = math.sqrt(vec2.map(x => x * x).sum)
  if (normA > 0 && normB > 0) dotProduct / (normA * normB) else 0.0
}

// Step 8: UDF for cosine similarity calculation
val cosineSimilarityUDF = udf((vec1: SparseVector, vec2: SparseVector) => cosineSimilarity(vec1.toArray, vec2.toArray))

// Step 9: Self-join for comparison
val joinedData = featurizedData.as("a")
  .join(featurizedData.as("b"), col("a.postal_cd") === col("b.postal_cd") && col("a.row_id") =!= col("b.row_id"))

// Step 10: Filter matches based on numeric and text similarity
val similarityData = joinedData
  .filter(col("a.address_digits") === col("b.address_digits")) // First check: numeric parts match
  .select(
    col("a.row_id").alias("RowID1"),
    col("b.row_id").alias("RowID2"),
    col("a.cleaned_address").alias("Address1"),
    col("b.cleaned_address").alias("Address2"),
    cosineSimilarityUDF(col("a.rawFeatures"), col("b.rawFeatures")).alias("SimilarityScore")
  )

// Step 11: Separate high and low similarity records
val highSimilarityData = similarityData.filter(col("SimilarityScore") > 0.7)
val lowSimilarityData = similarityData.filter(col("SimilarityScore") <= 0.7)

// Step 12: Show and save results
highSimilarityData.show(truncate = false)
lowSimilarityData.show(truncate = false)

// Save high similarity records
highSimilarityData.coalesce(1).write.mode("overwrite").csv("/edl/hdfs/address/high_similarity_output")

// Save low similarity records
lowSimilarityData.coalesce(1).write.mode("overwrite").csv("/edl/hdfs/address/low_similarity_output")

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.ml.feature.HashingTF
import org.apache.spark.ml.linalg.SparseVector

// Initialize SparkSession
val spark = SparkSession.builder.appName("Address Deduplication").getOrCreate()

// Load the data from the CSV file
val filePath = "mns/address/input/Site_Addrress_Validation-Dashboard_summary_old_data.csv"
val rawData = spark.read.option("header", "true").csv(filePath)

// Step 1: Add unique row_id using monotonically_increasing_id()
// Repartition to control the number of partitions and IDs
val rawDataWithID = rawData.repartition(1).withColumn("row_id", monotonically_increasing_id())

// Step 2: Remove extra commas from the address field
if (!rawData.columns.contains("address")) {
  throw new IllegalArgumentException("The input data does not have a column named 'address'. Please check the input file.")
}

val cleanAddressUDF = udf((address: String) => {
  if (address != null) address.replaceAll(",+", ",").trim.toLowerCase else ""
})
val cleanedData = rawDataWithID.withColumn("cleaned_address", cleanAddressUDF(col("address")))

// Step 3: Extract numeric and non-numeric portions of the address
val extractNumbersUDF = udf((address: String) => {
  if (address != null) address.replaceAll("[^0-9]", "") else ""
})
val extractNonNumbersUDF = udf((address: String) => {
  if (address != null) address.replaceAll("[0-9]", "").replaceAll(",", " ").trim.toLowerCase else ""
})
val extractedData = cleanedData
  .withColumn("address_digits", extractNumbersUDF(col("cleaned_address")))
  .withColumn("address_non_digits", extractNonNumbersUDF(col("cleaned_address")))
  .select("row_id", "postal_cd", "cleaned_address", "address_digits", "address_non_digits")

// Step 4: Split the non-numeric portion into tokens
val splitTokensUDF = udf((address: String) => {
  if (address != null) address.split(" ").map(_.trim).filter(_.nonEmpty) else Array.empty[String]
})
val tokenizedData = extractedData.withColumn("address_tokens", splitTokensUDF(col("address_non_digits")))

// Step 5: Use HashingTF for feature extraction
val hashingTF = new HashingTF()
  .setInputCol("address_tokens")
  .setOutputCol("rawFeatures")
  .setNumFeatures(1000)
val featurizedData = hashingTF.transform(tokenizedData)

// Step 6: Define cosine similarity function
def cosineSimilarity(vec1: Array[Double], vec2: Array[Double]): Double = {
  val dotProduct = vec1.zip(vec2).map { case (x, y) => x * y }.sum
  val normA = math.sqrt(vec1.map(x => x * x).sum)
  val normB = math.sqrt(vec2.map(x => x * x).sum)
  if (normA > 0 && normB > 0) dotProduct / (normA * normB) else 0.0
}

// Step 7: UDF for cosine similarity calculation
val cosineSimilarityUDF = udf((vec1: SparseVector, vec2: SparseVector) => cosineSimilarity(vec1.toArray, vec2.toArray))

// Step 8: Self-join for comparison
val joinedData = featurizedData.as("a")
  .join(featurizedData.as("b"), col("a.postal_cd") === col("b.postal_cd") && col("a.row_id") =!= col("b.row_id"))

// Step 9: Filter matches based on numeric and text similarity
val similarityData = joinedData
  .filter(col("a.address_digits") === col("b.address_digits")) // First check: numeric parts match
  .select(
    col("a.row_id").alias("RowID1"),
    col("b.row_id").alias("RowID2"),
    col("a.cleaned_address").alias("Address1"),
    col("b.cleaned_address").alias("Address2"),
    cosineSimilarityUDF(col("a.rawFeatures"), col("b.rawFeatures")).alias("SimilarityScore")
  )

// Step 10: Separate high and low similarity records
val highSimilarityData = similarityData.filter(col("SimilarityScore") > 0.7)
val lowSimilarityData = similarityData.filter(col("SimilarityScore") <= 0.7)

// Step 11: Ensure sum of unique and duplicate records matches total input records
val uniqueRecordCount = rawDataWithID.count()
val duplicateRecordCount = highSimilarityData.count() + lowSimilarityData.count()

if (uniqueRecordCount != duplicateRecordCount) {
  throw new IllegalStateException(s"Record count mismatch! Input: $uniqueRecordCount, Output: $duplicateRecordCount")
}

// Step 12: Save results
highSimilarityData.coalesce(1).write.mode("overwrite").csv("/edl/hdfs/address/high_similarity_output")
lowSimilarityData.coalesce(1).write.mode("overwrite").csv("/edl/hdfs/address/low_similarity_output")




import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.ml.feature.HashingTF
import org.apache.spark.ml.linalg.SparseVector

// Initialize SparkSession
val spark = SparkSession.builder.appName("Address Deduplication").getOrCreate()

// Function to load data and add a unique ID
def loadAndPreprocess(filePath: String): DataFrame = {
  val rawData = spark.read.option("header", "true").csv(filePath)
  
  // Add unique row_id and remove extra commas in the address column
  val cleanAddressUDF = udf((address: String) => {
    if (address != null) address.replaceAll(",+", ",").trim.toLowerCase else ""
  })
  
  rawData
    .repartition(1)
    .withColumn("row_id", monotonically_increasing_id())
    .withColumn("cleaned_address", cleanAddressUDF(col("address")))
}

// Replace suffixes with correct mappings
def replaceSuffixes(df: DataFrame): DataFrame = {
  val suffixMapping = Map(
    "ALLEE" -> "ALY", "ALLEY" -> "ALY", "ALLY" -> "ALY", "AV" -> "AVE",
    "AVENUE" -> "AVE", "BLVD" -> "BLVD", "STREET" -> "ST", "ROAD" -> "RD",
    "CIRCLE" -> "CIR", "DRIVE" -> "DR", "LANE" -> "LN", "COURT" -> "CT",
    "PARKWAY" -> "PKWY", "TERRACE" -> "TER", "PLACE" -> "PL"
    // Add more mappings as needed
  )
  
  val replaceSuffixUDF = udf((address: String) => {
    if (address != null) {
      suffixMapping.foldLeft(address) { case (updatedAddr, (oldSuffix, newSuffix)) =>
        updatedAddr.replaceAll(s"\\b$oldSuffix\\b", newSuffix)
      }
    } else ""
  })
  
  df.withColumn("standardized_address", replaceSuffixUDF(col("cleaned_address")))
}

// Extract numeric and non-numeric parts of the address
def extractAddressParts(df: DataFrame): DataFrame = {
  val extractNumbersUDF = udf((address: String) => {
    if (address != null) address.replaceAll("[^0-9]", "") else ""
  })
  val extractNonNumbersUDF = udf((address: String) => {
    if (address != null) address.replaceAll("[0-9]", "").replaceAll(",", " ").trim.toLowerCase else ""
  })
  
  df.withColumn("address_digits", extractNumbersUDF(col("standardized_address")))
    .withColumn("address_non_digits", extractNonNumbersUDF(col("standardized_address")))
}

// Compare addresses within the same ZIP code
def compareAddresses(df: DataFrame): DataFrame = {
  val splitTokensUDF = udf((address: String) => {
    if (address != null) address.split(" ").map(_.trim).filter(_.nonEmpty) else Array.empty[String]
  })

  val tokenizedData = df.withColumn("address_tokens", splitTokensUDF(col("address_non_digits")))

  // Feature extraction using HashingTF
  val hashingTF = new HashingTF()
    .setInputCol("address_tokens")
    .setOutputCol("rawFeatures")
    .setNumFeatures(1000)
  val featurizedData = hashingTF.transform(tokenizedData)

  // Cosine similarity UDF
  def cosineSimilarity(vec1: Array[Double], vec2: Array[Double]): Double = {
    val dotProduct = vec1.zip(vec2).map { case (x, y) => x * y }.sum
    val normA = math.sqrt(vec1.map(x => x * x).sum)
    val normB = math.sqrt(vec2.map(x => x * x).sum)
    if (normA > 0 && normB > 0) dotProduct / (normA * normB) else 0.0
  }

  val cosineSimilarityUDF = udf((vec1: SparseVector, vec2: SparseVector) => cosineSimilarity(vec1.toArray, vec2.toArray))

  // Self-join for comparison within the same ZIP code
  val joinedData = featurizedData.as("a")
    .join(featurizedData.as("b"), col("a.postal_cd") === col("b.postal_cd") && col("a.row_id") =!= col("b.row_id"))

  // Filter records with matching numeric portions and high similarity
  val similarityData = joinedData
    .filter(col("a.address_digits") === col("b.address_digits")) // Numeric parts match
    .select(
      col("a.row_id").alias("RowID1"),
      col("b.row_id").alias("RowID2"),
      col("a.standardized_address").alias("Address1"),
      col("b.standardized_address").alias("Address2"),
      cosineSimilarityUDF(col("a.rawFeatures"), col("b.rawFeatures")).alias("SimilarityScore")
    )
    .filter(col("SimilarityScore") > 0.7)

  similarityData
}

// Eliminate duplicate records and validate record counts
def eliminateDuplicates(inputDf: DataFrame, similarityDf: DataFrame): DataFrame = {
  val duplicateRowIds = similarityDf.select("RowID2").distinct()
  val uniqueRecords = inputDf.join(duplicateRowIds, inputDf("row_id") === duplicateRowIds("RowID2"), "left_anti")
  
  uniqueRecords
}

// Main execution
val filePath = "mns/address/input/Site_Addrress_Validation-Dashboard_summary_old_data.csv"
val rawData = loadAndPreprocess(filePath)
val standardizedData = replaceSuffixes(rawData)
val extractedData = extractAddressParts(standardizedData)
val similarityData = compareAddresses(extractedData)
val uniqueRecords = eliminateDuplicates(standardizedData, similarityData)

// Save unique and duplicate records, ensuring record counts match
val totalInputRecords = rawData.count()
val totalOutputRecords = uniqueRecords.count() + similarityData.count()

if (totalInputRecords != totalOutputRecords) {
  throw new IllegalStateException(s"Record count mismatch! Input: $totalInputRecords, Output: $totalOutputRecords")
}

uniqueRecords.coalesce(1).write.mode("overwrite").csv("/edl/hdfs/address/unique_records_output")
similarityData.coalesce(1).write.mode("overwrite").csv("/edl/hdfs/address/duplicate_records_output")


import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.ml.feature.HashingTF
import org.apache.spark.ml.linalg.SparseVector

// Initialize SparkSession
val spark = SparkSession.builder.appName("Address Deduplication").getOrCreate()

// Load the data
val filePath = "path/to/your/input.csv"
val rawData = spark.read.option("header", "true").csv(filePath)

// Step 1: Add unique row_id
val rawDataWithID = rawData.withColumn("row_id", monotonically_increasing_id())

// Step 2: Remove double commas from the address field
val cleanAddressUDF = udf((address: String) => address.replaceAll(",+", ",").trim)
val cleanedData = rawDataWithID.withColumn("cleaned_address", cleanAddressUDF(col("address")))

// Step 3: Extract numeric and non-numeric portions of the address
val extractNumbersUDF = udf((address: String) => address.replaceAll("[^0-9]", ""))
val extractNonNumbersUDF = udf((address: String) => address.replaceAll("[0-9]", "").replaceAll(",", " ").trim.toLowerCase)

val extractedData = cleanedData
  .withColumn("address_digits", extractNumbersUDF(col("cleaned_address")))
  .withColumn("address_non_digits", extractNonNumbersUDF(col("cleaned_address")))
  .select("row_id", "postal_cd", "cleaned_address", "address_digits", "address_non_digits")

// Step 4: Use HashingTF for feature extraction
val hashingTF = new HashingTF()
  .setInputCol("address_non_digits")
  .setOutputCol("rawFeatures")
  .setNumFeatures(1000)

val featurizedData = hashingTF.transform(extractedData)

// Step 5: Self-join for comparison
val joinedData = featurizedData.as("a")
  .join(featurizedData.as("b"),
    col("a.postal_cd") === col("b.postal_cd") &&
    col("a.row_id") < col("b.row_id") // Prevent duplicate comparisons
  )

// Step 6: Filter matches based on numeric and text similarity
def cosineSimilarity(vec1: Array[Double], vec2: Array[Double]): Double = {
  val dotProduct = vec1.zip(vec2).map { case (x, y) => x * y }.sum
  val normA = math.sqrt(vec1.map(x => x * x).sum)
  val normB = math.sqrt(vec2.map(x => x * x).sum)
  if (normA > 0 && normB > 0) dotProduct / (normA * normB) else 0.0
}

val cosineSimilarityUDF = udf((vec1: SparseVector, vec2: SparseVector) => cosineSimilarity(vec1.toArray, vec2.toArray))

val similarityData = joinedData
  .filter(col("a.address_digits") === col("b.address_digits")) // First check: numeric parts match
  .select(
    col("a.row_id").alias("RowID1"),
    col("b.row_id").alias("RowID2"),
    col("a.cleaned_address").alias("Address1"),
    col("b.cleaned_address").alias("Address2"),
    cosineSimilarityUDF(col("a.rawFeatures"), col("b.rawFeatures")).alias("SimilarityScore")
  )

// Step 7: Separate high and low similarity records
val highSimilarityData = similarityData.filter(col("SimilarityScore") > 0.7)
val lowSimilarityData = similarityData.filter(col("SimilarityScore") <= 0.7)

// Save the results
val uniqueData = featurizedData.join(highSimilarityData, featurizedData("row_id") === highSimilarityData("RowID1"), "leftanti")

highSimilarityData.coalesce(1).write.mode("overwrite").csv("path/to/high_similarity_output")
uniqueData.coalesce(1).write.mode("overwrite").csv("path/to/unique_output")


import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.ml.feature.HashingTF
import org.apache.spark.ml.linalg.SparseVector

// Initialize SparkSession
val spark = SparkSession.builder.appName("Address Deduplication with Mapping").getOrCreate()

// Load the data
val filePath = "path/to/your/input.csv"
val rawData = spark.read.option("header", "true").csv(filePath)

// Step 1: Add unique row_id
val rawDataWithID = rawData.withColumn("row_id", monotonically_increasing_id())

// Step 2: Remove double commas from the address field
val cleanAddressUDF = udf((address: String) => if (address != null) address.replaceAll(",+", ",").trim else address)
val cleanedData = rawDataWithID.withColumn("cleaned_address", cleanAddressUDF(col("address")))

// Step 3: Address Suffix Mapping
// Define mapping as a Map
val suffixMapping = Map(
  "ALLEE" -> "ALY", "ALLEY" -> "ALY", "ALLY" -> "ALY", "ANEX" -> "ANX", "ANNEX" -> "ANX",
  "AV" -> "AVE", "AVE" -> "AVE", "AVEN" -> "AVE", "AVENUE" -> "AVE", "BAYOU" -> "BYU",
  "BLVD" -> "BLVD", "BOULEVARD" -> "BLVD", "CIR" -> "CIR", "CIRCLE" -> "CIR", "CT" -> "CT",
  "DR" -> "DR", "DRIVE" -> "DR", "LN" -> "LN", "ROAD" -> "RD", "ST" -> "ST", "STREET" -> "ST"
  // Add more mappings as needed
)

// Broadcast the mapping to all nodes
val suffixBroadcast = spark.sparkContext.broadcast(suffixMapping)

// UDF to replace suffixes
val mapSuffixUDF = udf((address: String) => {
  if (address != null) {
    val words = address.split("\\s+")
    words.map(word => suffixBroadcast.value.getOrElse(word.toUpperCase, word)).mkString(" ")
  } else {
    address
  }
})

// Apply suffix mapping
val mappedData = cleanedData.withColumn("mapped_address", mapSuffixUDF(col("cleaned_address")))

// Step 4: Extract numeric and non-numeric portions of the address
val extractNumbersUDF = udf((address: String) => if (address != null) address.replaceAll("[^0-9]", "") else "")
val extractNonNumbersUDF = udf((address: String) => if (address != null) address.replaceAll("[0-9]", "").replaceAll(",", " ").trim.toLowerCase else "")

val extractedData = mappedData
  .withColumn("address_digits", extractNumbersUDF(col("mapped_address")))
  .withColumn("address_non_digits", extractNonNumbersUDF(col("mapped_address")))

// Step 5: Use HashingTF for feature extraction
val hashingTF = new HashingTF()
  .setInputCol("address_non_digits")
  .setOutputCol("rawFeatures")
  .setNumFeatures(1000)

val featurizedData = hashingTF.transform(extractedData)

// Step 6: Self-join for comparison
val joinedData = featurizedData.as("a")
  .join(featurizedData.as("b"),
    col("a.postal_cd") === col("b.postal_cd") &&
    col("a.row_id") < col("b.row_id") // Prevent duplicate comparisons
  )

// Step 7: Filter matches based on numeric and text similarity
def cosineSimilarity(vec1: Array[Double], vec2: Array[Double]): Double = {
  val dotProduct = vec1.zip(vec2).map { case (x, y) => x * y }.sum
  val normA = math.sqrt(vec1.map(x => x * x).sum)
  val normB = math.sqrt(vec2.map(x => x * x).sum)
  if (normA > 0 && normB > 0) dotProduct / (normA * normB) else 0.0
}

val cosineSimilarityUDF = udf((vec1: SparseVector, vec2: SparseVector) => cosineSimilarity(vec1.toArray, vec2.toArray))

val similarityData = joinedData
  .filter(col("a.address_digits") === col("b.address_digits")) // First check: numeric parts match
  .select(
    col("a.row_id").alias("RowID1"),
    col("b.row_id").alias("RowID2"),
    col("a.mapped_address").alias("Address1"),
    col("b.mapped_address").alias("Address2"),
    cosineSimilarityUDF(col("a.rawFeatures"), col("b.rawFeatures")).alias("SimilarityScore")
  )

// Step 8: Separate high and low similarity records
val highSimilarityData = similarityData.filter(col("SimilarityScore") > 0.7)
val lowSimilarityData = similarityData.filter(col("SimilarityScore") <= 0.7)

// Step 9: Identify unique records
val uniqueData = featurizedData.join(highSimilarityData, featurizedData("row_id") === highSimilarityData("RowID1"), "leftanti")

// Save the results
highSimilarityData.coalesce(1).write.mode("overwrite").csv("path/to/high_similarity_output")
lowSimilarityData.coalesce(1).write.mode("overwrite").csv("path/to/low_similarity_output")
uniqueData.coalesce(1).write.mode("overwrite").csv("path/to/unique_output")

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.ml.feature.HashingTF
import org.apache.spark.ml.linalg.SparseVector

// Initialize SparkSession
val spark = SparkSession.builder
  .appName("Address Deduplication and Tokenization")
  .getOrCreate()

// Load the data from the CSV file
val filePath = "path/to/input.csv"
val rawData = spark.read.option("header", "true").csv(filePath)

// Step 1: Add unique row_id
val rawDataWithID = rawData.withColumn("row_id", monotonically_increasing_id())

// Step 2: Remove extra commas
val cleanAddressUDF = udf((address: String) => if (address != null) address.replaceAll(",+", ",").trim else null)
val cleanedData = rawDataWithID.withColumn("cleaned_address", cleanAddressUDF(col("address")))

// Step 3: Replace suffixes using a mapping
val suffixMapping = Map(
  "AV" -> "AVE", "AVEN" -> "AVE", "AVENUE" -> "AVE", "ALLEY" -> "ALY", "BLVD" -> "BLVD",
  "STREET" -> "ST", "ROAD" -> "RD", "DRIVE" -> "DR", "LANE" -> "LN", "CIRCLE" -> "CIR"
)

val replaceSuffixUDF = udf((address: String) => {
  if (address != null) {
    suffixMapping.foldLeft(address)((acc, mapping) => acc.replaceAll("\\b" + mapping._1 + "\\b", mapping._2))
  } else null
})

val mappedData = cleanedData.withColumn("mapped_address", replaceSuffixUDF(col("cleaned_address")))

// Step 4: Split the address into components
val extractComponentsUDF = udf((address: String) => {
  if (address != null) {
    val parts = address.split(",").map(_.trim)
    val digits = """^\d+""".r.findFirstIn(parts.head).getOrElse("")
    val streetName = parts.head.replaceFirst("""^\d+\s*""", "").trim
    val city = if (parts.length > 1) parts(1) else ""
    val stateZip = if (parts.length > 2) parts(2).split("\\s+") else Array("", "")
    val state = if (stateZip.nonEmpty) stateZip.head else ""
    val zip = if (stateZip.length > 1) stateZip.tail.mkString(" ") else ""
    val country = if (parts.length > 3) parts(3) else ""
    Array(digits, streetName, city, state, zip, country)
  } else Array("", "", "", "", "", "")
})

val splitColumns = Seq("initial_digits", "street_name", "city", "state", "zipcode", "country")
val tokenizedData = mappedData
  .withColumn("address_components", extractComponentsUDF(col("mapped_address")))
  .select(col("*") +: splitColumns.zipWithIndex.map { case (colName, idx) =>
    col("address_components").getItem(idx).alias(colName)
  }: _*)

// Step 5: Tokenize the non-numeric portion of the address
val tokenizeUDF = udf((text: String) => if (text != null) text.split("\\s+").filter(_.nonEmpty) else Array.empty[String])
val featurizedData = tokenizedData.withColumn("address_tokens", tokenizeUDF(col("street_name")))

// Step 6: Use HashingTF for feature extraction
val hashingTF = new HashingTF()
  .setInputCol("address_tokens")
  .setOutputCol("rawFeatures")
  .setNumFeatures(1000)

val hashedData = hashingTF.transform(featurizedData)

// Step 7: Self-join to compare addresses within the same postal code
val joinedData = hashedData.as("a")
  .join(hashedData.as("b"), col("a.postal_cd") === col("b.postal_cd") && col("a.row_id") < col("b.row_id"))

// Step 8: Calculate similarity score using cosine similarity
def cosineSimilarity(vec1: Array[Double], vec2: Array[Double]): Double = {
  val dotProduct = vec1.zip(vec2).map { case (x, y) => x * y }.sum
  val normA = math.sqrt(vec1.map(x => x * x).sum)
  val normB = math.sqrt(vec2.map(x => x * x).sum)
  if (normA > 0 && normB > 0) dotProduct / (normA * normB) else 0.0
}

val cosineSimilarityUDF = udf((vec1: SparseVector, vec2: SparseVector) => cosineSimilarity(vec1.toArray, vec2.toArray))

val similarityData = joinedData
  .filter(col("a.address_digits") === col("b.address_digits")) // Compare numeric parts
  .withColumn("SimilarityScore", cosineSimilarityUDF(col("a.rawFeatures"), col("b.rawFeatures")))

// Step 9: Filter records by similarity score
val highSimilarityData = similarityData.filter(col("SimilarityScore") > 0.7)

// Exclude duplicates from original data
val uniqueRecords = hashedData.except(highSimilarityData.select(col("a.*")))

// Duplicate records for review
val duplicateRecords = highSimilarityData.select(
  col("a.row_id").alias("RowID1"),
  col("b.row_id").alias("RowID2"),
  col("a.mapped_address").alias("Address1"),
  col("b.mapped_address").alias("Address2"),
  col("SimilarityScore")
)

// Step 10: Write unique and duplicate records to single CSV files
uniqueRecords
  .drop("address_tokens", "rawFeatures")
  .coalesce(1)
  .write.mode("overwrite").option("header", "true").csv("path/to/unique_records_output")

duplicateRecords
  .coalesce(1)
  .write.mode("overwrite").option("header", "true").csv("path/to/duplicate_records_output")

