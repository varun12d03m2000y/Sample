24/12/30 12:44:31 INFO  spark.SparkContext: [main]: Created broadcast 5 from csv at <pastie>:57
24/12/30 12:44:31 INFO  execution.FileSourceScanExec: [main]: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
24/12/30 12:44:32 INFO  datasources.FileSourceScanPlan: [main]: Pushed Filters: 
24/12/30 12:44:32 INFO  datasources.FileSourceScanPlan: [main]: Post-Scan Filters: 
24/12/30 12:44:32 INFO  datasources.FileSourceScanPlan: [main]: Output Data Schema: struct<address: string, postal_cd: string, aiops_site_id: string ... 1 more fields>
24/12/30 12:44:32 INFO  datasources.FileSourceScanPlan: [main]: Pushed Filters: 
24/12/30 12:44:32 INFO  datasources.FileSourceScanPlan: [main]: Post-Scan Filters: 
24/12/30 12:44:32 INFO  datasources.FileSourceScanPlan: [main]: Output Data Schema: struct<address: string, postal_cd: string, aiops_site_id: string ... 1 more fields>
24/12/30 12:44:32 INFO  datasources.FileSourceScanPlan: [main]: Pushed Filters: 
24/12/30 12:44:32 INFO  datasources.FileSourceScanPlan: [main]: Post-Scan Filters: 
24/12/30 12:44:32 INFO  datasources.FileSourceScanPlan: [main]: Output Data Schema: struct<address: string, postal_cd: string, aiops_site_id: string ... 1 more fields>
org.apache.spark.sql.AnalysisException: CSV data source does not support array<string> data type.


    // Write non-duplicate results
    val flatNonDuplicateDf = nonDuplicateDf
      .withColumn("filtered_tokens_str", concat_ws(" ", col("filtered_tokens")))
      .drop("filtered_tokens")

    flatNonDuplicateDf.write.mode("overwrite").option("header", "true").csv(mainOutputPath)

    // Write duplicate results
    val flatDuplicateDf = duplicateDf
      .withColumn("filtered_tokens_str", concat_ws(" ", col("filtered_tokens")))
      .drop("filtered_tokens")

    flatDuplicateDf.write.mode("overwrite").option("header", "true").csv(duplicateOutputPath)

    println(s"Main results have been written to $mainOutputPath")
    println(s"Duplicate records have been written to $duplicateOutputPath")
  }
}

Schema of nonDuplicateDf before writing:
root
 |-- address: string (nullable = true)
 |-- postal_cd: string (nullable = true)
 |-- aiops_site_id: string (nullable = true)
 |-- normalized_address: string (nullable = true)
 |-- normalized_postal_cd: string (nullable = true)
 |-- tokens: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filtered_tokens: array (nullable = true)
 |    |-- element: string (containsNull = true)

Schema of duplicateDf before writing:
root
 |-- aiops_site_id_1: string (nullable = true)
 |-- aiops_site_id_2: string (nullable = true)
 |-- similarity_score: double (nullable = true)
