import org.apache.spark.sql.functions._
import org.apache.spark.sql.{SparkSession, Row}
import org.apache.spark.ml.feature.{StopWordsRemover, Tokenizer}
import scala.collection.mutable.ArrayBuffer

object AddressMatching {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Address Matching")
      .master("local[*]")
      .getOrCreate()

    // Input path
    val inputPath = "/edl/hdfs/jffv-mns/testaddress/input/query-impala-88364.csv"
    val data = spark.read.option("header", "true").csv(inputPath)

    // Normalize addresses
    val normalizeUDf = udf((address: String) => {
      Option(address).map(_.toLowerCase
        .replaceAll("\\b(st|road|rd|avenue|ave|boulevard|blvd|way|circle|pk|park)\\b", "st")
        .replaceAll("[^a-z0-9\\s]", "")
        .trim).getOrElse("")
    })

    val normalizedDf = data
      .withColumn("normalized_address", normalizeUDf(col("address")))
      .withColumn("normalized_postal_cd", col("postal_cd").substr(1, 5))

    // Tokenize and remove stopwords
    val tokenizer = new Tokenizer().setInputCol("normalized_address").setOutputCol("tokens")
    val tokenizedDf = tokenizer.transform(normalizedDf)

    val remover = new StopWordsRemover().setInputCol("tokens").setOutputCol("filtered_tokens")
    val filteredDf = remover.transform(tokenizedDf)

    // Group by postal code and create address groups
    val groupedDf = filteredDf.groupBy("normalized_postal_cd").agg(
      collect_list(struct("normalized_address", "filtered_tokens")).as("address_group")
    )

    // UDF to compute Jaccard similarity
    val similarityUDF = udf((addresses: Seq[Row]) => {
      val results = ArrayBuffer[(String, String, Double)]()
      for (i <- addresses.indices; j <- i + 1 until addresses.size) {
        val addr1 = addresses(i)
        val addr2 = addresses(j)
        val tokens1 = addr1.getAs[Seq[String]]("filtered_tokens")
        val tokens2 = addr2.getAs[Seq[String]]("filtered_tokens")
        val intersection = tokens1.intersect(tokens2).size.toDouble
        val union = tokens1.union(tokens2).distinct.size.toDouble
        val score = if (union > 0) intersection / union else 0.0
        results += ((addr1.getAs[String]("normalized_address"), addr2.getAs[String]("normalized_address"), score))
      }
      results
    })

    // Compute comparisons
    val comparisonDf = groupedDf.withColumn("comparisons", similarityUDF(col("address_group")))

    // Flatten comparisons and create a new DataFrame with address pairs and their similarity score
    val flatDf = comparisonDf
      .select(explode(col("comparisons")).as("comparison"))
      .select(
        col("comparison._1").as("address_1"),
        col("comparison._2").as("address_2"),
        col("comparison._3").as("similarity_score")
      )

    // Filter duplicates: similarity_score = 1.0 (identical addresses)
    val duplicateDf = flatDf.filter(col("similarity_score") === 1.0)

    // Get the distinct addresses from the duplicate list
    val duplicateAddresses = duplicateDf
      .select(col("address_2"))
      .union(duplicateDf.select(col("address_1")))
      .distinct()

    // Filter out duplicates from the original dataset
    val nonDuplicateDf = filteredDf.join(
      duplicateAddresses,
      filteredDf("normalized_address") === duplicateAddresses("address_2"),
      "left_anti"
    )

    // Flatten the tokens and filtered_tokens columns into strings
    val flatNonDuplicateDf = nonDuplicateDf
      .withColumn("tokens_str", concat_ws(" ", col("tokens")))
      .withColumn("filtered_tokens_str", concat_ws(" ", col("filtered_tokens")))
      .drop("tokens", "filtered_tokens") // Drop the original array columns

    // Output paths
    val similarityOutputPath = "/edl/hdfs/jffv-mns/testaddress/output/address_similarity.csv"
    val uniqueAddressOutputPath = "/edl/hdfs/jffv-mns/testaddress/output/unique_addresses.csv"

    // Write address similarity results
    flatDf.write
      .mode("overwrite")
      .option("header", "true")
      .csv(similarityOutputPath)

    // Write unique address records
    flatNonDuplicateDf.write
      .mode("overwrite")
      .option("header", "true")
      .csv(uniqueAddressOutputPath)

    // Count duplicates removed
    val duplicateCount = duplicateAddresses.count()

    // Print how many duplicates were removed
    println(s"Number of duplicate addresses removed: $duplicateCount")
    println(s"Address similarity results have been written to $similarityOutputPath")
    println(s"Unique address records have been written to $uniqueAddressOutputPath")
  }
}
