import org.apache.spark.sql.functions._
import org.apache.spark.sql.{SparkSession, Row}
import org.apache.spark.ml.feature.{StopWordsRemover, Tokenizer}
import scala.collection.mutable.ArrayBuffer

object AddressMatching {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Address Matching")
      .master("local[*]")
      .getOrCreate()

    // Input path
    val inputPath = "/edl/hdfs/jffv-mns/testaddress/input/query-impala-88364.csv"
    val data = spark.read.option("header", "true").csv(inputPath)

    // Normalize addresses
    val normalizeUDf = udf((address: String) => {
      Option(address).map(_.toLowerCase
        .replaceAll("\\b(st|road|rd|avenue|ave|boulevard|blvd|way|circle|pk|park)\\b", "st")
        .replaceAll("[^a-z0-9\\s]", "")
        .trim).getOrElse("")
    })

    val normalizedDf = data
      .withColumn("normalized_address", normalizeUDf(col("address")))
      .withColumn("normalized_postal_cd", col("postal_cd").substr(1, 5))

    // Tokenize and remove stopwords
    val tokenizer = new Tokenizer().setInputCol("normalized_address").setOutputCol("tokens")
    val tokenizedDf = tokenizer.transform(normalizedDf)

    val remover = new StopWordsRemover().setInputCol("tokens").setOutputCol("filtered_tokens")
    val filteredDf = remover.transform(tokenizedDf)

    // Group by postal code and create address groups
    val groupedDf = filteredDf.groupBy("normalized_postal_cd").agg(
      collect_list(struct("normalized_address", "filtered_tokens")).as("address_group")
    )

    // UDF to compute Jaccard similarity
    val similarityUDF = udf((addresses: Seq[Row]) => {
      val results = ArrayBuffer[(String, String, Double)]()
      for (i <- addresses.indices; j <- i + 1 until addresses.size) {
        val addr1 = addresses(i)
        val addr2 = addresses(j)
        val tokens1 = addr1.getAs[Seq[String]]("filtered_tokens")
        val tokens2 = addr2.getAs[Seq[String]]("filtered_tokens")
        val intersection = tokens1.intersect(tokens2).size.toDouble
        val union = tokens1.union(tokens2).distinct.size.toDouble
        val score = if (union > 0) intersection / union else 0.0
        results += ((addr1.getAs[String]("normalized_address"), addr2.getAs[String]("normalized_address"), score))
      }
      results
    })

    // Compute comparisons
    val comparisonDf = groupedDf.withColumn("comparisons", similarityUDF(col("address_group")))

    // Flatten comparisons and create a new DataFrame with address pairs and their similarity score
    val flatDf = comparisonDf
      .select(explode(col("comparisons")).as("comparison"))
      .select(
        col("comparison._1").as("address_1"),
        col("comparison._2").as("address_2"),
        col("comparison._3").as("similarity_score")
      )

    // Filter duplicates: similarity_score = 1.0 (identical addresses)
    val duplicateDf = flatDf.filter(col("similarity_score") === 1.0)

    // Get the distinct addresses from the duplicate list
    val duplicateAddresses = duplicateDf
      .select(col("address_2"))
      .union(duplicateDf.select(col("address_1")))
      .distinct()

    // Filter out duplicates from the original dataset
    val nonDuplicateDf = filteredDf.join(
      duplicateAddresses,
      filteredDf("normalized_address") === duplicateAddresses("address_2"),
      "left_anti"
    )

    // Flatten the tokens and filtered_tokens columns into strings
    val flatNonDuplicateDf = nonDuplicateDf
      .withColumn("tokens_str", concat_ws(" ", col("tokens")))
      .withColumn("filtered_tokens_str", concat_ws(" ", col("filtered_tokens")))
      .drop("tokens", "filtered_tokens") // Drop the original array columns

    // Output paths
    val similarityOutputPath = "/edl/hdfs/jffv-mns/testaddress/output/address_similarity.csv"
    val uniqueAddressOutputPath = "/edl/hdfs/jffv-mns/testaddress/output/unique_addresses.csv"

    // Write address similarity results
    flatDf.write
      .mode("overwrite")
      .option("header", "true")
      .csv(similarityOutputPath)

    // Write unique address records
    flatNonDuplicateDf.write
      .mode("overwrite")
      .option("header", "true")
      .csv(uniqueAddressOutputPath)

    // Count duplicates removed
    val duplicateCount = duplicateAddresses.count()

    // Print how many duplicates were removed
    println(s"Number of duplicate addresses removed: $duplicateCount")
    println(s"Address similarity results have been written to $similarityOutputPath")
    println(s"Unique address records have been written to $uniqueAddressOutputPath")
  }
}




List(
  "ALLEE" -> "ALY", "ALLEY" -> "ALY", "ALLY" -> "ALY", "ANEX" -> "ANX", "ANNEX" -> "ANX",
  "ARC" -> "ARC", "ARCADE" -> "ARC", "AV" -> "AVE", "AVE" -> "AVE", "AVEN" -> "AVE",
  "AVENU" -> "AVE", "AVENUE" -> "AVE", "AVN" -> "AVE", "AVNUE" -> "AVE", "BAYOO" -> "BYU",
  "BAYOU" -> "BYU", "BCH" -> "BCH", "BEACH" -> "BCH", "BEND" -> "BND", "BLF" -> "BLF",
  "BLUF" -> "BLF", "BLUFF" -> "BLF", "BLUFFS" -> "BLFS", "BOT" -> "BTM", "BOTTM" -> "BTM",
  "BOTTOM" -> "BTM", "BLVD" -> "BLVD", "BOUL" -> "BLVD", "BOULEVARD" -> "BLVD", "BOULV" -> "BLVD",
  "BR" -> "BR", "BRNCH" -> "BR", "BRANCH" -> "BR", "BRDGE" -> "BR", "BRK" -> "BR",
  "BROOK" -> "BR", "BROOKS" -> "BR", "BURG" -> "BR", "BURGS" -> "BR", "BYP" -> "BYP",
  "BYPA" -> "BYP", "BYPAS" -> "BYP", "BYPASS" -> "BYP", "BYPS" -> "BYP", "CAMP" -> "CMP",
  "CANYN" -> "CYN", "CANYON" -> "CYN", "CNYN" -> "CYN", "CAPE" -> "CPE", "CAUSEWAY" -> "CSWY",
  "CEN" -> "CTR", "CENTER" -> "CTR", "CENTR" -> "CTR", "CENTRE" -> "CTR", "CNTER" -> "CTR",
  "CNTR" -> "CTR", "CTR" -> "CTR", "CENTERS" -> "CTR", "CIR" -> "CIR", "CIRC" -> "CIR",
  "CIRCL" -> "CIR", "CIRCLE" -> "CIR", "CRCL" -> "CIR", "CRCLE" -> "CIR", "CIRCLES" -> "CIR",
  "CLF" -> "CLF", "CLIFF" -> "CLF", "CLFS" -> "CLF", "CLIFFS" -> "CLF", "CLB" -> "CLB",
  "CLUB" -> "CLB", "COMMON" -> "CMN", "COMMONS" -> "CMN", "COR" -> "COR", "CORNER" -> "COR",
  "CORNERS" -> "COR", "COURSE" -> "CRSE", "COURT" -> "CT", "COURTS" -> "CT", "COVE" -> "CV",
  "CREEK" -> "CRK", "CRESCENT" -> "CRES", "CREST" -> "CRST", "CROSSING" -> "XING", "CROSSROAD" -> "XRD",
  "CROSSROADS" -> "XRDS", "CURVE" -> "CURV", "DALE" -> "DL", "DAM" -> "DM", "DIV" -> "DV",
  "DIVIDE" -> "DV", "DR" -> "DR", "DRIV" -> "DR", "DRIVE" -> "DR", "DRV" -> "DR", "DRIVES" -> "DR",
  "EST" -> "EST", "ESTATE" -> "EST", "ESTATES" -> "EST", "EXP" -> "EXP", "EXPRESSWAY" -> "EXPY",
  "EXT" -> "EXT", "EXTENSION" -> "EXT", "FALL" -> "FAL", "FALLS" -> "FLS", "FERRY" -> "FRY",
  "FIELD" -> "FLD", "FIELDS" -> "FLDS", "FLAT" -> "FLT", "FORD" -> "FRD", "FORG" -> "FRG",
  "FORGE" -> "FRG", "FORK" -> "FRK", "FORKS" -> "FRK", "FORT" -> "FT", "FREEWAY" -> "FWY",
  "GARDEN" -> "GDN", "GARDENS" -> "GDS", "GATEWAY" -> "GTWY", "GLEN" -> "GLN", "GLENS" -> "GLNS",
  "GREEN" -> "GRN", "GREENS" -> "GRNS", "GROVE" -> "GRV", "GROVES" -> "GRVS", "HARBOR" -> "HBR",
  "HARBORS" -> "HBR", "HAVEN" -> "HVN", "HILL" -> "HL", "HILLS" -> "HLS", "HOLLOWS" -> "HOLW",
  "ISLAND" -> "IS", "ISLANDS" -> "ISLND", "ISLE" -> "ISLE", "JCT" -> "JCT", "JUNCTION" -> "JCT",
  "KEY" -> "KY", "LAKE" -> "LK", "LAKES" -> "LKS", "LANDING" -> "LNDG", "LANE" -> "LN",
  "LIGHT" -> "LGT", "LOAF" -> "LF", "LOCK" -> "LCK", "LOCKS" -> "LCK", "LODGE" -> "LDG",
  "LOOP" -> "LOOP", "MALL" -> "MAL", "MANOR" -> "MNR", "MILL" -> "ML", "MILLS" -> "MLS",
  "MEADOW" -> "MDW", "MEADOWS" -> "MDWS", "MOTORWAY" -> "MTWY", "MOUNT" -> "MT", "MOUNTAIN" -> "MTN",
  "NECK" -> "NCK", "ORCHARD" -> "ORCH", "OVERPASS" -> "OPAS", "PARK" -> "PARK", "PARKWAY" -> "PKWY",
  "PASSAGE" -> "PSGE", "PIKE" -> "PIKE", "PINE" -> "PNE", "PINE" -> "PNE", "PINES" -> "PNES",
  "PLAIN" -> "PLN", "PLAZA" -> "PLZ", "POINT" -> "PT", "PORT" -> "PRT", "PRAIRIE" -> "PRR",
  "RAMP" -> "RAMP", "RANCH" -> "RNCH", "RAPID" -> "RPD", "RAPIDS" -> "RPDS", "REST" -> "RST",
  "RIDGE" -> "RDG", "RIVER" -> "RIV", "ROAD" -> "RD", "ROUTE" -> "RTE", "ROW" -> "ROW", "RUN" -> "RUN",
  "SHORE" -> "SHR", "SHORE" -> "SHR", "SKYWAY" -> "SKWY", "SPRING" -> "SPRG", "SPUR" -> "SPUR",
  "SQUARE" -> "SQ", "STATION" -> "STA", "STREET" -> "ST", "SUMMIT" -> "SUMMIT", "TER" -> "TER",
  "TERRACE" -> "TERR", "TRACE" -> "TRCE", "TRAIL" -> "TRL", "TUNNEL" -> "TUNL", "UNION" -> "UN",
  "VALLEY" -> "VLY", "VILLAGE" -> "VLG", "VIEW" -> "VW", "WALK" -> "WALK", "WAY" -> "WAY"
)

import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._

object AddressStandardization {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Address Standardization")
      .master("local[*]")  // Replace with your cluster settings if necessary
      .getOrCreate()

    // Debugging: Set log level to DEBUG for more details in Spark logs
    spark.sparkContext.setLogLevel("DEBUG")

    import spark.implicits._

    // Define the lists for street suffix mapping
    val suffixMapping = Map(
      "ALLEE" -> "ALY",
      "ALLEY" -> "ALY",
      "ALLY" -> "ALY",
      "ANEX" -> "ANX",
      "AV" -> "AVE",
      "BLVD" -> "BOULEVARD",
      "CIR" -> "CIRCLE",
      "CT" -> "COURT",
      "AVE" -> "AVENUE",
      "PL" -> "PLACE",
      "RD" -> "ROAD",
      "ST" -> "STREET",
      "LN" -> "LANE",
      "TRAIL" -> "TRAILER",
      "PARKWAY" -> "PKWY",
      "BLF" -> "BLUFF",
      "HILLS" -> "HL",
      "RANCH" -> "RNCH",
      "SQUARE" -> "SQ",
      "GROVE" -> "GRV",
      "HEIGHTS" -> "HTS",
      "CIRCLE" -> "CIR"
      // Add more abbreviations and mappings as needed
    )

    // Function to standardize addresses using suffix mapping
    val standardizeSuffixUDF = udf((address: String) => {
      if (address == null || address.isEmpty) address
      else {
        val words = address.split("\\s+")
        words.map(word => suffixMapping.getOrElse(word.toUpperCase, word)).mkString(" ")
      }
    })

    // Read input CSV
    val inputPath = "path_to_input_file.csv" // Replace with your input file path
    val outputUniquePath = "path_to_unique_addresses.csv" // Replace with output path for unique records
    val outputDuplicatesPath = "path_to_duplicate_addresses.csv" // Replace with output path for duplicates
    val outputStandardizedPath = "path_to_standardized_addresses.csv" // Path for standardized addresses

    val df = spark.read
      .option("header", "true")
      .csv(inputPath)
      .withColumn("standardized_address", standardizeSuffixUDF(col("address")))

    // Debugging: Print input DataFrame count and preview some rows
    println(s"Input DataFrame Row Count: ${df.count()}")
    df.show(5)  // Show first 5 rows to confirm the input data

    // Create a DataFrame with just the standardized address
    val standardizedDF = df.select("standardized_address")

    // Find duplicate addresses (addresses having more than one record)
    val duplicatesDF = standardizedDF.groupBy("standardized_address")
      .count()
      .filter($"count" > 1)
      .drop("count")

    // Debugging: Print duplicate DataFrame count and preview
    println(s"Duplicate DataFrame Row Count: ${duplicatesDF.count()}")
    duplicatesDF.show(5)

    // Write duplicate addresses to CSV, overwriting if necessary
    duplicatesDF.write
      .option("header", "true")
      .mode("overwrite") // Overwrite the existing file
      .csv(outputDuplicatesPath)

    // Now find unique addresses by subtracting duplicates from the original DataFrame
    val uniqueDF = standardizedDF.except(duplicatesDF)

    // Debugging: Print unique DataFrame count and preview
    println(s"Unique DataFrame Row Count: ${uniqueDF.count()}")
    uniqueDF.show(5)

    // Write unique addresses to CSV, overwriting if necessary
    uniqueDF.write
      .option("header", "true")
      .mode("overwrite") // Overwrite the existing file
      .csv(outputUniquePath)

    // Write standardized addresses to CSV, overwriting if necessary
    df.write
      .option("header", "true")
      .mode("overwrite") // Overwrite the existing file
      .csv(outputStandardizedPath)

    // Print summary of record counts
    println(s"Input Records: ${df.count()}")
    println(s"Unique Records: ${uniqueDF.count()}")
    println(s"Duplicate Records: ${duplicatesDF.count()}")
    println(s"Sum (Unique + Duplicate): ${uniqueDF.count() + duplicatesDF.count()}")

    spark.stop()
  }
}

